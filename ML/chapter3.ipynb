{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3、线性回归算法"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.1、线性回归基本概念\n",
    "用线性模型来拟合特征组与标签之间**回归关系**的方法就称为线性回归\n",
    "\n",
    "机器学习将一个形如 $h_{w,b}(x) = <w,x> + b$ 的$R^n \\rightarrow R$ 的函数称为一个线性模型。其中$w, x \\in R^n$为n维向量，$b \\in R$为偏置项，$<w, x>$表示w和x的内积。\n",
    "\n",
    "求解线性回归模型中参数$(w, b)$的问题称为线性回归问题。\n",
    "\n",
    "一般情况下，线性回归算法实际上是一个经验损失最小化算法。模型假设为线性模型，损失函数为平方损失函数。对于数据集$(x, y)$，模型h的平方损失为$(h(x)-y)^2$.算法描述如下：\n",
    "\n",
    "样本空间 $X \\subseteq R^n$\n",
    "\n",
    "输入：m条训练数据$S = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(n)}, y^{(n)})\\}$\n",
    "\n",
    "输出：线性模型$h_{w^*,b^*}(x)=<w^*, x> + b^*$，使得$w^*, b^*$为优化问题$\\min _ { w \\in \\mathbb { R } ^ { n } , b \\in \\mathbb { R } } \\frac { 1 } { m } \\sum _ { i = 1 } ^ { m } \\left( \\left\\langle w , x ^ { ( i ) } \\right\\rangle + b - y ^ { ( i ) } \\right) ^ { 2 }$的最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### **定义3.1(均方误差)**\n",
    "$\\frac { 1 } { m } \\left( \\left\\langle w , x ^ { ( i ) } \\right\\rangle + b - y ^ { ( i ) } \\right) ^ { 2 }$\n",
    "\n",
    "### **定义3.2(似然函数)**\n",
    "给定随机变量，定义\n",
    "Like $\\left( w | y ^ { ( 1 ) } , y ^ { ( 2 ) } , \\cdots , y ^ { ( m ) } \\right) = \\prod _ { i = 1 } ^ { m } p _ { w } \\left( Y = y ^ { ( i ) } \\right)$\n",
    "\n",
    "为Y的m个独立采样恰为$y^{(1)}, y^{(2)}, \\cdots, y^{(m)}$的概率，称其为概率分布$p_w$关于$y^{(1)}, y^{(2)}, \\cdots, y^{(m)}$的似然函数\n",
    "\n",
    "概率中的最大似然原则：如果$y^{(1)}, y^{(2)}, \\cdots, y^{(m)}$为Y的m个独立采样，而$w^*$是使得似然函数最大的一组参数，即\n",
    "\n",
    "$w ^ { * } = \\underset { w \\in \\mathbb { R } ^ { n } } { \\operatorname { argmax } } \\operatorname { Like } \\left( w | y ^ { ( 1 ) } , y ^ { ( 2 ) } , \\cdots , y ^ { ( m ) } \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.2线性回归优化算法\n",
    "### 定理3.1 线性回归的均值误差\n",
    "$F \\left( w \\right) = \\frac {1} {m} \\sum _ { i=1 } ^ { m } \\left( \\left< w, x^{(i)} \\right> - y^{(i)} \\right) ^ 2 $ 是一个关于w的可微**凸函数**，从而线性回归问题是一个凸优化问题。因此线性回归有唯一最优值。\n",
    "\n",
    "$w \\in R^n $最小化均方误差$F \\left( w \\right)$的充要条件是梯度$\\bigtriangledown F \\left( w \\right) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    def fit(self, X, y):\n",
    "        self.w=np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "    def predect(self, X):\n",
    "        return X.dot(self.w)\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.average((y_true-y_pred)**2, axis=0)\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    numerator=(y_true-y_pred)**2\n",
    "    denominator=(y_true-np.average(y_true, axis=0))**2\n",
    "    return 1-numerator.sum(axis=0)/denominator.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
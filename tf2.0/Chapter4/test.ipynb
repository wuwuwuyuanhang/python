{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbasecondab708f16640e64910af49310a539c91f5",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.1 数据类型\n",
    "### 4.1.1 数值类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(float, tensorflow.python.framework.ops.EagerTensor, True)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1.2 # python语言创建标量\n",
    "aa = tf.constant(1.2) # tf方式创建标量\n",
    "type(a), type(aa), tf.is_tensor(aa)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "通过`print(x)`或`x`的方式可以打印出张量x的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=1, shape=(3,), dtype=float32, numpy=array([1. , 2. , 3.3], dtype=float32)>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([1, 2., 3.3])\n",
    "x # 打印TF张量的相关信息"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "其中id是Tensorflow中内部索引对象的编号，shape表示张量的形状，dtype表示张量的数字精确度，张量numpy()方法可以返回Numpy.array类型的数据，方便导出数据到系统的其他模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1. , 2. , 3.3], dtype=float32)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "与标量不同，向量的定义须通过List容器传给tf.constant()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: id=2, shape=(1,), dtype=float32, numpy=array([1.2], dtype=float32)>,\n TensorShape([1]))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个元素的向量\n",
    "a = tf.constant([1.2])\n",
    "a, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: id=3, shape=(3,), dtype=float32, numpy=array([1., 2., 3.], dtype=float32)>,\n TensorShape([3]))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建三个元素的向量\n",
    "a = tf.constant([1, 2, 3.])\n",
    "a, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: id=4, shape=(2, 2), dtype=int32, numpy=\n array([[1, 2],\n        [3, 4]])>, TensorShape([2, 2]))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用同样的方法，定义矩阵的实现\n",
    "a = tf.constant([[1, 2], [3, 4]]) # 创建2行2列的矩阵\n",
    "a, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: id=5, shape=(2, 2, 2), dtype=int32, numpy=\n array([[[1, 2],\n         [3, 4]],\n \n        [[5, 6],\n         [7, 8]]])>, TensorShape([2, 2, 2]))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建三维张量\n",
    "a = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "a, a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.1.2 字符串类型\n",
    "通过传入字符串对象即可创建字符串类型的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=6, shape=(), dtype=string, numpy=b'Hello, Deep Learning.'>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant('Hello, Deep Learning.')\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "在tf.strings模块中，提供了常见的字符串工具函数，如小写化lower()，拼接join(),长度length(),切分split()等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=7, shape=(), dtype=string, numpy=b'hello, deep learning.'>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.lower(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.1.3 布尔类型\n",
    "TensorFlow还支持布尔类型的张量。布尔类型张量只需传入Python语言的布尔类型，转换成TensorFlow内部布尔型即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=8, shape=(), dtype=bool, numpy=True>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant(True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=9, shape=(2,), dtype=bool, numpy=array([ True, False])>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建布尔型向量\n",
    "a = tf.constant([True, False])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorFlow的布尔型和Python语言的布尔型**并不等价**，不能通用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "False\ntf.Tensor(True, shape=(), dtype=bool)\n"
    }
   ],
   "source": [
    "a = tf.constant(True) # 创建TF布尔类型\n",
    "print(a is True) # TF布尔类型张量与python布尔型比较\n",
    "print(a == True) # 仅值比较"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 数值精度\n",
    "对于数值类型的张量，可以保存不同字节长度的精确度，如浮点型数3.14既可以保存为16位(Bit)长度，也可以保存为32位甚至64位精确度。位越长，精确度越高，同时占用的内存空间也越大。常用的精度类型有tf.int16、tf.int32、tf.int64、tf.float16、tf.float32、tf.float64等，其中tf.float64即为tf.double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=13, shape=(), dtype=int16, numpy=-13035>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在创建张量时，可以指定张量的保存精度\n",
    "tf.constant(123456789, dtype=tf.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=14, shape=(), dtype=int32, numpy=123456789>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(123456789, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "可以看出，保存精度过低时，数据123456789发生了溢出，得到了错误的结果，一搬使用tf.int32、tf.float64精度。对于浮点数，高精度的张量可以表示更精确的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.2.1 读取精度\n",
    "通过访问张量的dtype成员属性可以判断张量的保存精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "before: <dtype: 'bool'>\nafter: <dtype: 'float32'>\n"
    }
   ],
   "source": [
    "print('before:', a.dtype) # 读取原有张量的数值精度\n",
    "if a.dtype != tf.float32:\n",
    "    a = tf.cast(a, tf.float32) # 使用tf.cast函数可以\n",
    "print('after:', a.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.2.2 类型转换\n",
    "类型转换需通过tf.cast函数完成，进行类型转换时需要保证转换操作的合法性，例如将高精度的张量转换成为低精度的张量时，可能发生数据溢出隐患"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.3 待优化张量\n",
    "通过tf.Variable()可以将普通张量转换为待优化张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "('Variable:0', True)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([-1, 0, 1, 2]) # 创建TF张量\n",
    "aa = tf.Variable(a) # 转为Variable类型张量\n",
    "aa.name, aa.trainable # Variable张量的属性"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "除了通过普通张量方式创建Variable，也可以直接创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]])>\n"
    }
   ],
   "source": [
    "a = tf.Variable([[1, 2], [3, 4]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "待优化张量可以视为普通张量的特殊类型，普通张量其实也可以通过GradientTape.watch()方法临时加入跟踪梯度信息的列表，从而支持自动求导功能"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.4 创建张量\n",
    "### 4.4.1 从数组、列表对象创建\n",
    "通过tf.convert_to_tensor函数可以创建新Tensor，并将保存在Python List对象或者Numpy Array对象中的数据带入到新的Tensor中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=32, shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.convert_to_tensor([1, 2.]) # 从列表创建张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=33, shape=(2, 2), dtype=float64, numpy=\narray([[1., 2.],\n       [3., 4.]])>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tf.convert_to_tensor(np.array([[1, 2.], [3, 4]])) #从数组中创建张量"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "需要注意的是，Numpy浮点数组默认使用64位精度保存数据，转换到Tensor类型时精度为tf.float64，可以在需要的时候将其转换为tf.float32类型"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.4.2 创建全0或全1长张量\n",
    "将张量创建为全0或者全1数据是非常常见的张量初始化手段。考虑线性变换y = Wx + b ，将权值矩阵 W 初始化为全1矩阵，偏置 b 初始化为全0向量，此时线性变化层输出 y = x ，因此是一种比较好的层初始化状态。通过tf.zeros()和tf.ones()即可创建任意形状，且内容全0或全1的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: id=34, shape=(), dtype=float32, numpy=0.0>,\n <tf.Tensor: id=35, shape=(), dtype=float32, numpy=1.0>)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros([]), tf.ones([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: id=38, shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Tensor: id=41, shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建全0和全1的向量\n",
    "tf.zeros([1]), tf.ones([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: id=44, shape=(2, 2), dtype=float32, numpy=\n array([[0., 0.],\n        [0., 0.]], dtype=float32)>,\n <tf.Tensor: id=47, shape=(3, 2), dtype=float32, numpy=\n array([[1., 1.],\n        [1., 1.],\n        [1., 1.]], dtype=float32)>)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros([2, 2]), tf.ones([3, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "通过tf.zeros_like，tf.ones_like可以方便的新建与某个张量shape一致，且内容为全0或者全1的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=51, shape=(2, 3), dtype=float32, numpy=\narray([[0., 0., 0.],\n       [0., 0., 0.]], dtype=float32)>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.ones([2, 3])\n",
    "tf.zeros_like(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.4.3 创建自定义数值张量\n",
    "通过tf.fill(shape, value)可以创建全为自定义数值value的张量，形状由shape参数指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=54, shape=(), dtype=int32, numpy=-1>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.fill([], -1) # 创建-1的标量"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.4.4 创建已知分布的张量\n",
    "正态分布和均值分布是最常见的分布之一，创建采样自这2种分布的张量非常有用，比如在卷积神经网络中，卷积核张量W初始化为正态分布；在对抗生产网络中，隐藏层变量z一般采样自均值分布\n",
    "\n",
    "通过tf.random.normal(shape, mean=0.0, stddev=1.0)可以创建形状为shape，均值为mean，标准差为stddev的正态分布N。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=60, shape=(2, 2), dtype=float32, numpy=\narray([[ 1.0869708 , -0.08471203],\n       [-0.40201572,  0.226951  ]], dtype=float32)>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.normal([2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "通过tf.random.uniform(shape, minval=0, maxval=None, dtype=tf.float32)可以创建采样自[minval, maxval)区间的均匀分布的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=67, shape=(2, 2), dtype=float32, numpy=\narray([[0.899817  , 0.84741616],\n       [0.94304895, 0.22638023]], dtype=float32)>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.uniform([2, 2]) # 创建采用自[0, 1)的均匀分布矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=74, shape=(2, 2), dtype=float32, numpy=\narray([[9.640602 , 8.145672 ],\n       [7.021474 , 3.8338327]], dtype=float32)>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.uniform([2, 2], maxval=10) # 创建采样自[0, 10)均匀分布矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.4.5 创建序列\n",
    "在循环计算或者对张量进行索引时，经常需要创建一段连续的整数序列，可以通过tf.range()函数实现。\n",
    "\n",
    "tf.range(limit, delta=1)可以创建[0, limit)之间，步长为delta的整型序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=78, shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=82, shape=(5,), dtype=int32, numpy=array([1, 3, 5, 7, 9])>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(1, 10, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.5 张量的典型应用\n",
    "### 4.5.1 标量\n",
    "标量就是一个简单的数字，维度为0，shape为[]，标量的一些典型用途是**误差率的表示**、各种**测试指标**的表示\n",
    "\n",
    "以均方差误差函数为例，经过tf.keras.losses.mse返回每个样本上的误差值，最后取误差的均值作为当前Batch的误差，它只是一个标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor(0.35754168, shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "out = tf.random.uniform([4, 10]) # 随机模拟网络输出\n",
    "y = tf.constant([2, 3, 2, 0]) # 随机构造样本真实标签\n",
    "y = tf.one_hot(y, depth=10) # one-hot编码\n",
    "loss = tf.keras.losses.mse(y, out) # 计算每个样本的MSE\n",
    "loss = tf.reduce_mean(loss) # 平均MSE，loss应是标量\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.5.2 向量\n",
    "偏置b就使用向量表示，把所有输出节点的偏置表示成向量形式 $ b = \\left[ b_1, b_2 \\right] ^ T $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=109, shape=(4, 2), dtype=float32, numpy=\narray([[ 0.35497734, -1.0715697 ],\n       [-0.30867717,  0.5965641 ],\n       [ 1.0800284 ,  2.0894403 ],\n       [-1.392141  , -0.7874128 ]], dtype=float32)>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z = wx, 模拟获得激活函数的输入z\n",
    "z = tf.random.normal([4, 2])\n",
    "b = tf.zeros([2]) # 创建偏置向量\n",
    "z = z + b # 累加上偏置向量\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "通过高层接口类Dense()方式创建的网络层，张量W和b存储在类的内部，由类自动创建并管理。可以通过全连接层的bias成员变量查看偏置b，例如创建输入节点数为4，输出节点数为3的线性层网络，那么它的偏置向量b的长度应为3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc = tf.keras.layers.Dense(3) # 创建一层Wx+b，输出节点数为3\n",
    "# 通过build函数创建W, b张量，输入节点为4\n",
    "fc.build(input_shape=(2, 4))\n",
    "fc.bias # 查看偏置向量"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.5.3 矩阵\n",
    "矩阵也是非常常见的张量类型，比如全连接层的批量输入张量X的形状为 $ \\left[b, d_{in} \\right]$，其中b表示输入样本的个数，即Batch_Size, $ d_{in} $ 表示输入特征的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=147, shape=(2, 3), dtype=float32, numpy=\narray([[-0.66265523, -0.66265523, -0.66265523],\n       [ 1.2417035 ,  1.2417035 ,  1.2417035 ]], dtype=float32)>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal([2, 4]) # 2个样本，特征长度为4的张量\n",
    "w = tf.ones([4, 3]) # 定义W张量\n",
    "b = tf.zeros([3]) # 定义b张量\n",
    "o = x@w + b\n",
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "在TensorFlow中可以通过Dense类直接实现全连接层 $ \\sigma \\left ( X@W + b \\right ) $，特别地，当激活函数 $ \\sigma $ 为空时，全连接层也称为线性层。我们可以通过Dense类创建输入4个节点，输出3个节点的网络层，并通过全连接层的Kernel成员名查看其权值矩阵W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Variable 'kernel:0' shape=(4, 3) dtype=float32, numpy=\narray([[ 0.07031351, -0.7961218 , -0.83275414],\n       [-0.8902934 ,  0.08401394,  0.10655653],\n       [-0.00588936, -0.8820179 , -0.09860969],\n       [ 0.2403338 , -0.839215  ,  0.5954622 ]], dtype=float32)>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc = tf.keras.layers.Dense(3) # 定义全连接层的输出节点数为3\n",
    "fc.build(input_shape=(2, 4))\n",
    "fc.kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.5.4 三维张量\n",
    "三维的张量一个典型应用是表示序列信号，它的格式是 X = [b, sequence len, feature len] 其中b表示序列信号的数量，sequence len表示序列信号在时间维度上的采样点数或步数，feature len表示每个点的特征长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n17465344/17464789 [==============================] - 8s 0us/step\n"
    },
    {
     "data": {
      "text/plain": "(25000, 80)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=10000) # 自动加载IMDB电影评价数据集\n",
    "# 将句子填充、截断为等长80个单词的句子\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=80)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([25000, 80, 100])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建词向量Embedding层类\n",
    "embedding = tf.keras.layers.Embedding(10000, 100)\n",
    "# 将数字编码的单词转换为词向量\n",
    "out = embedding(x_train)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.5.5 四维张量\n",
    "四维张量在卷积神经网络中应用十分广泛，它用于保存特征图(Feature maps)数据，格式一般定义为 [b, h, w, c] 其中b表示输入样本的数量，h/w分别表示特征图的高/宽，c表示特征图的通道数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([4, 30, 30, 16])"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal([4, 32, 32, 3]) # 创建32 * 32 的彩色图片输入个数为4\n",
    "layer = keras.layers.Conv2D(16, kernel_size=3) # 创建卷积神经网络\n",
    "out = layer(x) # 前向计算\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([3, 3, 3, 16])"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.kernel.shape # 访问卷积核张量"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.6 索引与切片\n",
    "\n",
    "通过索引与切片操作可以提取张量的部分数据，使用频率非常高\n",
    "\n",
    "### 4.6.1 索引\n",
    "\n",
    "在TensorFlow中支持基本的[i][j]...标准索引方式，也支持通过逗号分隔索引号的索引方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor(\n[[[ 0.9358451  -0.18618539 -0.85742205]\n  [-1.4369293   2.3945692  -0.13809584]\n  [-0.07258236 -0.49016306  1.615732  ]\n  ...\n  [ 0.8562781  -0.18121845  0.90331864]\n  [ 0.10782529 -1.2000215   0.64827996]\n  [ 1.019487    0.5614674   0.3659705 ]]\n\n [[ 0.5786742   0.57937765 -1.2847228 ]\n  [-0.33243424 -0.42127576  0.98879635]\n  [ 1.6769586  -0.32224828  0.73290235]\n  ...\n  [-0.3431775   0.7716607   0.8982969 ]\n  [ 0.43976325  0.9296968  -0.9666902 ]\n  [-0.79846686 -0.82033885 -0.7452553 ]]\n\n [[ 0.26795456  0.3958743  -0.12063207]\n  [ 0.6451852   2.3712933   1.3862005 ]\n  [ 0.5227435   0.59213984 -0.34021226]\n  ...\n  [-0.52888215  0.417686    0.18460533]\n  [ 0.01772973 -0.04185007 -0.8194715 ]\n  [-1.027557   -1.5185268   1.1515623 ]]\n\n ...\n\n [[-0.71366316 -0.5502357  -0.12019509]\n  [ 1.7647713  -0.32990968  0.29064897]\n  [-1.2607658   0.23626927  0.52281284]\n  ...\n  [ 2.0099618  -1.5285137   0.537436  ]\n  [-0.71538156  0.31361958 -0.763305  ]\n  [ 1.9095011   0.42584723 -0.45282137]]\n\n [[-1.3523837   1.8693513  -0.01393805]\n  [ 0.59956425 -1.3274593  -1.5800581 ]\n  [-0.22566183  0.37003684 -0.3612337 ]\n  ...\n  [ 0.1449733  -0.58893436  0.7592434 ]\n  [-1.176762   -1.3432022  -0.5452478 ]\n  [-0.5992153   0.20926756 -1.1014613 ]]\n\n [[ 1.3139869  -0.7348386  -0.26863828]\n  [-0.4061586  -1.1921346  -0.08315775]\n  [ 0.3475897   0.40965906  0.56752616]\n  ...\n  [-0.03898275 -0.07531507  0.4577493 ]\n  [ 0.16396184 -1.5085733   1.189154  ]\n  [ 1.6852413   0.87098676 -0.12445387]]], shape=(32, 32, 3), dtype=float32)\ntf.Tensor(\n[[ 0.5786742   0.57937765 -1.2847228 ]\n [-0.33243424 -0.42127576  0.98879635]\n [ 1.6769586  -0.32224828  0.73290235]\n [-1.4264438  -0.44502524 -0.5255421 ]\n [-0.42174175  0.91153866  1.0684    ]\n [-0.12914476 -1.741715   -1.97309   ]\n [ 1.021091   -0.37047634  0.43173036]\n [-1.6419787   0.45066738 -0.745548  ]\n [ 0.14579073  1.0173135   0.04345693]\n [ 0.25652057 -0.15662406 -1.0776752 ]\n [ 0.6562833   0.7896303   1.7861083 ]\n [ 0.03483859 -0.6829478  -0.8986556 ]\n [ 0.8804144   0.96006685  0.4217412 ]\n [ 0.01661682  0.0043959   0.894099  ]\n [-0.04097218 -0.03426493  0.6235022 ]\n [-0.09524933 -0.3219071   0.4672082 ]\n [ 0.31388167 -1.1270338  -0.8474214 ]\n [ 1.6566477  -0.6469857   1.4630511 ]\n [-0.3152234   0.7750208  -0.54835653]\n [-1.6691958   0.5882168   1.659532  ]\n [ 0.25035602 -0.6505558   2.9628613 ]\n [ 0.96200395 -0.728017   -0.8763717 ]\n [ 0.62011164  1.5137244   1.2109773 ]\n [ 1.476694   -1.5084757  -0.701817  ]\n [ 0.9332195  -0.52240264 -0.5572281 ]\n [-0.70779926  0.26171985  0.89018977]\n [ 2.6909559   0.8171862  -1.4543686 ]\n [-0.28098     0.762394   -0.36525533]\n [-0.23842065  0.9544366  -0.87731856]\n [-0.3431775   0.7716607   0.8982969 ]\n [ 0.43976325  0.9296968  -0.9666902 ]\n [-0.79846686 -0.82033885 -0.7452553 ]], shape=(32, 3), dtype=float32)\ntf.Tensor([ 1.6769586  -0.32224828  0.73290235], shape=(3,), dtype=float32)\ntf.Tensor(-0.49753708, shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "x = tf.random.normal([4, 32, 32, 3])\n",
    "print(x[0])\n",
    "print(x[0][1])\n",
    "print(x[0][1][2])\n",
    "print(x[2][1][0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.6.2 切片\n",
    "通过start:end:step切片方式可以方便地提取一段数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=316, shape=(2, 32, 32, 3), dtype=float32, numpy=\narray([[[[ 0.2536348 ,  0.9613802 , -0.74066764],\n         [ 1.3441887 , -0.68319774, -0.06843416],\n         [-1.2493814 , -1.0101233 , -0.7906423 ],\n         ...,\n         [-0.09210271,  1.5779675 , -0.32729697],\n         [-0.16231447,  0.4092217 , -0.5080269 ],\n         [ 1.112306  ,  0.19783218,  0.2649804 ]],\n\n        [[-0.6841071 , -1.2246782 ,  0.22186257],\n         [ 0.04325636, -0.13493343, -0.17881139],\n         [-0.67875564,  0.8408532 ,  0.220963  ],\n         ...,\n         [-0.27084795, -0.07742538,  1.1287125 ],\n         [-0.42117113, -0.0686627 ,  0.6794488 ],\n         [-1.1624132 , -0.00828036,  0.97831666]],\n\n        [[ 0.29933023,  0.26944834, -0.7728486 ],\n         [ 0.3971441 , -0.46672922,  0.9659381 ],\n         [-0.2515187 ,  0.944085  , -0.3354304 ],\n         ...,\n         [ 1.4515959 ,  1.6836886 ,  0.43171793],\n         [ 1.1990772 , -0.2698902 ,  0.15674701],\n         [-0.34955895,  0.7193056 ,  0.53051454]],\n\n        ...,\n\n        [[ 0.628549  , -1.2288321 ,  0.27912152],\n         [ 1.3875239 ,  0.6761919 ,  0.13886479],\n         [-0.9837382 ,  0.80019915,  0.51245576],\n         ...,\n         [ 0.8121615 ,  0.3905309 , -2.1295664 ],\n         [-0.28643322,  0.04530345,  0.6744321 ],\n         [-1.4333596 ,  1.7188818 ,  0.7654983 ]],\n\n        [[-0.79900265, -0.8980509 ,  0.7565607 ],\n         [-1.4274232 ,  0.4331843 ,  0.13329089],\n         [-1.2166914 ,  1.8172357 ,  0.58994716],\n         ...,\n         [-0.00859733, -1.3517042 ,  2.0689232 ],\n         [-0.9968364 ,  1.3834968 , -0.00389402],\n         [-0.50883865,  0.27231714,  0.17882368]],\n\n        [[ 0.54130137,  0.7135187 , -1.1038283 ],\n         [-0.5728867 ,  0.29896557, -0.1915188 ],\n         [-0.7974263 ,  1.2446876 ,  0.30410042],\n         ...,\n         [-0.22547205,  0.7256305 , -0.6102524 ],\n         [ 0.24195617,  0.4139425 , -0.8415353 ],\n         [-0.5511334 , -0.19384849, -0.8582506 ]]],\n\n\n       [[[-1.924825  ,  2.047735  ,  0.9651472 ],\n         [-0.00991211,  1.0835857 ,  0.9435877 ],\n         [-0.5507946 ,  0.1852547 , -0.87657654],\n         ...,\n         [-0.12539943,  0.77450246, -1.4915688 ],\n         [ 1.2087412 , -2.180345  , -1.1113397 ],\n         [-0.18704511,  0.96298   , -0.5997307 ]],\n\n        [[ 1.0760363 , -0.49753708, -0.23328169],\n         [ 1.2597432 , -0.31664482, -1.1795585 ],\n         [-0.38085464,  0.19117965, -1.414203  ],\n         ...,\n         [-1.3761865 ,  1.1759552 , -0.55328214],\n         [ 0.78776115, -0.44098258,  0.46189028],\n         [-0.96672714,  0.27608865, -2.7553926 ]],\n\n        [[ 1.7550222 ,  0.09786333, -1.4328176 ],\n         [-0.492671  ,  0.2479276 , -0.5179958 ],\n         [ 0.33120576,  0.28976142, -1.5392365 ],\n         ...,\n         [ 0.23481213,  0.2439776 , -2.095741  ],\n         [ 0.33270136,  0.2852401 , -0.47095686],\n         [ 1.0136155 ,  1.1506517 , -0.29913184]],\n\n        ...,\n\n        [[ 1.1909591 , -0.8032049 , -0.41177216],\n         [-0.31678393, -0.17000009,  1.2288855 ],\n         [ 0.2115197 ,  0.54798484,  1.9969764 ],\n         ...,\n         [-0.8831628 , -0.42150566, -1.9055582 ],\n         [ 0.05551889,  1.1930673 ,  0.8621601 ],\n         [-0.28382757,  0.11225551,  0.5226576 ]],\n\n        [[ 0.25052148,  0.22558483, -1.1764059 ],\n         [ 0.8802082 ,  0.7394695 , -1.3654437 ],\n         [-0.5741513 ,  2.5756087 ,  0.39745852],\n         ...,\n         [-0.24766469, -0.61859256,  0.6517494 ],\n         [-1.0648509 , -0.7363829 , -0.6573058 ],\n         [ 2.1682148 , -0.4245514 , -0.6045214 ]],\n\n        [[ 0.444011  ,  1.0721895 , -1.2906114 ],\n         [ 0.36377165,  0.24790388,  0.873555  ],\n         [-0.28982836, -0.08616167, -1.1142863 ],\n         ...,\n         [ 1.6622388 , -1.7894405 ,  0.5777055 ],\n         [ 1.1424476 , -0.6337198 , -1.7316364 ],\n         [ 0.5463972 ,  0.42405835, -0.1217126 ]]]], dtype=float32)>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start:end:step切片方式有很多简写方式，其中start、end、step 3个参数可以跟据需要选择性地省略，全部省略时即为::，表示从最开始读取到最末尾，步长为1。如x[0,::]表示读取第一张图片的所有行，等价于x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}